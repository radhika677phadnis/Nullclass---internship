{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b1b97-fcbd-4137-a0c9-2364650d63d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TASK NO. 5TH HERE I HAVE USED PRETRAINED DATA TO MAKE THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80a3f42-6964-4324-9a31-b7c2e64cc17d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file  \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\HP\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2/6.2MB 19.3MB/s 0.3s\n",
      "\n",
      "0: 640x640 2 persons, 2 cars, 1 traffic light, 187.7ms\n",
      "Speed: 17.8ms preprocess, 187.7ms inference, 15.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Done. Saved as result.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "BLUE_L, BLUE_U = np.array([90,50,50]), np.array([140,255,255])\n",
    "\n",
    "def detect(path):\n",
    "    img = cv2.imread(path)\n",
    "    res = model(img)[0]\n",
    "    for b in res.boxes:\n",
    "        x1,y1,x2,y2 = map(int,b.xyxy[0]); cls = res.names[int(b.cls)]\n",
    "        if cls in ['car','truck','bus']:\n",
    "            crop = img[y1:y2,x1:x2]\n",
    "            hsv = cv2.cvtColor(crop,cv2.COLOR_BGR2HSV)\n",
    "            blue = cv2.inRange(hsv,BLUE_L,BLUE_U).mean()>25\n",
    "            color = (0,0,255) if blue else (255,0,0)\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),color,2)\n",
    "        elif cls=='person':\n",
    "            import cv2, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "BLUE_L, BLUE_U = np.array([90,50,50]), np.array([140,255,255])\n",
    "\n",
    "def detect(path):\n",
    "    img = cv2.imread(path)\n",
    "    res = model(img)[0]\n",
    "    for b in res.boxes:\n",
    "        x1,y1,x2,y2 = map(int,b.xyxy[0]); cls = res.names[int(b.cls)]\n",
    "        if cls in ['car','truck','bus']:\n",
    "            crop = img[y1:y2,x1:x2]\n",
    "            hsv = cv2.cvtColor(crop,cv2.COLOR_BGR2HSV)\n",
    "            blue = cv2.inRange(hsv,BLUE_L,BLUE_U).mean()>25\n",
    "            color = (0,0,255) if blue else (255,0,0)\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),color,2)\n",
    "        elif cls=='person':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9adcf-3ba1-4e70-9f4f-238bc85fbcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np\n",
    "from ultralytics import YOLO\n",
    "from tkinter import Tk, filedialog, Label, Button\n",
    "from PIL import Image, ImageTk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "model = YOLO(\"yolov8n.pt\")\n",
    "BLUE_L, BLUE_U = np.array([90,50,50]), np.array([140,255,255])\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "def detect(path, true_label=None):\n",
    "    img = cv2.imread(path)\n",
    "    res = model(img)[0]\n",
    "    pred_label = \"none\"\n",
    "    for b in res.boxes:\n",
    "        x1,y1,x2,y2 = map(int,b.xyxy[0]); cls = res.names[int(b.cls)]\n",
    "        if cls in ['car','truck','bus']:\n",
    "            crop = img[y1:y2,x1:x2]\n",
    "            hsv = cv2.cvtColor(crop,cv2.COLOR_BGR2HSV)\n",
    "            blue = cv2.inRange(hsv,BLUE_L,BLUE_U).mean()>25\n",
    "            pred_label = \"blue\" if blue else \"not_blue\"\n",
    "            color = (0,0,255) if blue else (255,0,0)\n",
    "            cv2.rectangle(img,(x1,y1),(x2,y2),color,2)\n",
    "    if true_label:\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)\n",
    "    cv2.imwrite(\"result.jpg\",img)\n",
    "    return \"result.jpg\"\n",
    "\n",
    "def show_matrix():\n",
    "    if y_true and y_pred:\n",
    "        cm = confusion_matrix(y_true,y_pred,labels=[\"blue\",\"not_blue\",\"none\"])\n",
    "        disp = ConfusionMatrixDisplay(cm,display_labels=[\"blue\",\"not_blue\",\"none\"])\n",
    "        disp.plot(cmap=\"Blues\")\n",
    "        plt.show()\n",
    "\n",
    "def open_file():\n",
    "    file = filedialog.askopenfilename(filetypes=[(\"Images\",\"*.jpg *.png\")])\n",
    "    if file:\n",
    "        out = detect(file)\n",
    "        img = Image.open(out)\n",
    "        img = img.resize((400,300))\n",
    "        tkimg = ImageTk.PhotoImage(img)\n",
    "        lbl.config(image=tkimg)\n",
    "        lbl.image = tkimg\n",
    "\n",
    "root = Tk()\n",
    "root.title(\"Car Detection\")\n",
    "btn = Button(root,text=\"Upload Image\",command=open_file)\n",
    "btn.pack()\n",
    "lbl = Label(root)\n",
    "lbl.pack()\n",
    "btn2 = Button(root,text=\"Show Matrix\",command=show_matrix)\n",
    "btn2.pack()\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98788904-c7cd-4147-aca7-7f96e1599e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirement texts\n",
    "\n",
    "#opencv\n",
    "#Numpy\n",
    "#Ultralytics\n",
    "#pillow\n",
    "#matplotlib\n",
    "#scikit-learn\n",
    "#tk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8994e7-139e-40e3-ade5-3994556f70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\HP\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m imgs, labels \u001b[38;5;241m=\u001b[39m imgs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m out \u001b[38;5;241m=\u001b[39m model(imgs)\n\u001b[1;32m---> 29\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     31\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1310\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1309\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\torch\\nn\\functional.py:3462\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3461\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3462\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3463\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3466\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Long but found Int"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 3, 128, 128).astype(np.float32)\n",
    "y = np.random.randint(0, 6, 500)\n",
    "\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = models.resnet18(pretrained=False)\n",
    "model.fc = nn.Linear(model.fc.in_features, 6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = loss_fn(out, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"task5_color_model.pt\")\n",
    "print(\"Model saved as task5_color_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20325085-c90f-4a64-b136-fa72123e5667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as task5_color_model.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(500, 3, 128, 128).astype(np.float32)\n",
    "y = np.random.randint(0, 6, 500).astype(np.int64)\n",
    "\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "dataset = torch.utils.data.TensorDataset(X_tensor, y_tensor)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = models.resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, 6)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=1e-4)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    for imgs, labels in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        out = model(imgs)\n",
    "        loss = loss_fn(out, labels)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "torch.save(model.state_dict(), \"task5_color_model.pt\")\n",
    "print(\"Model saved as task5_color_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f355f8-02eb-44dc-9b5b-fc055e7571b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (longhairenv)",
   "language": "python",
   "name": "longhairenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
