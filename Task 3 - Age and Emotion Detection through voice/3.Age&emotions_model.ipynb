{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2840710-e836-49c9-a350-8983628ebcc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task number 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54d088ad-5428-4dff-b05f-36dd8e0a7803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For GUI, in jupyternotebbok it will add a button to add audios and interact with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a125ba8-3863-4e43-adc6-2b802cf045e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\envs\\tfenv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "C:\\Users\\HP\\anaconda3\\envs\\tfenv\\python.exe\n",
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\python.exe\n",
      "C:\\Users\\HP\\AppData\\Local\\Microsoft\\WindowsApps\\python.exe\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa soundfile\n",
    "!where python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4745a4c6-93da-4954-9b49-92fcb23fbaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: librosa in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: soundfile in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (0.61.2)\n",
      "Requirement already satisfied: numpy>=1.22.3 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.7.1)\n",
      "Requirement already satisfied: joblib>=1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.5.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (0.5.0.post1)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (4.14.1)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from librosa) (1.1.1)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from soundfile) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.22)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from pooch>=1.1->librosa) (4.3.8)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.8.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install librosa soundfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cfbd257-b016-40f1-9edf-f78f9cac0e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (8.1.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipywidgets) (8.37.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipywidgets) (4.0.14)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipywidgets) (3.0.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.14.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\hp\\anaconda3\\envs\\longhairenv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b778b159-5d34-4c52-834f-26ebcbe9539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(file):\n",
    "    y, sr = librosa.load(file, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "                   \n",
    "base =r\"C:\\Users\\HP\\Downloads\\Dataset for audios\"   \n",
    "X, y = [], []\n",
    "\n",
    "for label in os.listdir(base):\n",
    "    folder = os.path.join(base, label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".wav\"):\n",
    "            path = os.path.join(folder, f)\n",
    "            feat = extract_features(path)\n",
    "            emo_code = int(f.split(\"-\")[2])   \n",
    "            X.append(feat)\n",
    "            y.append(emo_code - 1)            \n",
    "\n",
    "print(\"Total samples loaded:\", len(X))\n",
    "print(\"Unique emotion labels:\", set(y))\n",
    "\n",
    "\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y, num_classes=8)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(40,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train,\n",
    "          validation_data=(X_test, y_test),\n",
    "          epochs=30, batch_size=32)\n",
    "\n",
    "\n",
    "model.save(\"emotion_model.h5\")\n",
    "print(\" Model saved as emotion_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2bdbc9-8e60-4f83-b641-8cd39a9ed614",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If female voice= upload in male voice\n",
    "#If age is >60 = senior citizen and emotions\n",
    "#If <60= detection age\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a31df98-7b07-4294-96ed-ef947aa61367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def extract_features(file):\n",
    "    y, sr = librosa.load(file, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "\n",
    "\n",
    "base = r\"C:\\Users\\HP\\Downloads\\Dataset for audios\"   \n",
    "X, y = [], []\n",
    "\n",
    "for label in os.listdir(base):\n",
    "    folder = os.path.join(base, label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "    for f in os.listdir(folder):\n",
    "        if f.endswith(\".wav\"):\n",
    "            path = os.path.join(folder, f)\n",
    "            feat = extract_features(path)\n",
    "            emo_code = int(f.split(\"-\")[2])  \n",
    "            X.append(feat)\n",
    "            y.append(emo_code - 1)            \n",
    "\n",
    "print(\"Total samples loaded:\", len(X))\n",
    "print(\"Unique emotion labels found:\", set(y))\n",
    "\n",
    "X = np.array(X)\n",
    "y = to_categorical(y, num_classes=8)   \n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(40,)),              \n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation='softmax')   \n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "\n",
    "model.save(\"emotion_model.h5\")\n",
    "model.save(\"emotion_model.keras\")\n",
    "print(\"Model saved as emotion_model.h5 and emotion_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0245d-be3c-42d7-9f09-28ee47697440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "def extract_features(file, sr=16000, n_mfcc=40):\n",
    "    y, sr = librosa.load(file, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d16566-41fc-45e1-828c-39eedd5577e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "gender_base = r\"C:\\Users\\HP\\Downloads\\AUDIOS\\audio_speech_actors_01-24\"  \n",
    "\n",
    "X, y = [], []\n",
    "import re\n",
    "\n",
    "label_map = {\"male\": 0, \"female\": 1}\n",
    "\n",
    "for label in os.listdir(gender_base):\n",
    "    folder = os.path.join(gender_base, label)\n",
    "    if not os.path.isdir(folder):\n",
    "        continue\n",
    "\n",
    "    \n",
    "    actor_id = int(re.search(r\"\\d+\", label).group())\n",
    "    gender = \"male\" if actor_id % 2 != 0 else \"female\"\n",
    "\n",
    "   validation_data=(X_test, y_test), epochs=30, batch_size=32)\n",
    "gender_model.save(\"gender_model.keras for f in os.listdir(folder):\n",
    "        if f.endswith(\".wav\"):\n",
    "            feat = extract_features(os.path.join(folder, f))\n",
    "            X.append(feat)\n",
    "            y.append(label_map[gender])\n",
    "\n",
    "\n",
    "X, y = np.array(X), np.array(y)\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "gender_model = Sequential([\n",
    "    Input(shape=(40,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "gender_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "gender_model.fit(X_train, y_train, \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c984c-8200-426c-93c2-d62ec682818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "base = r\"C:\\Users\\HP\\Downloads\\AUDIOS\\audio_speech_actors_01-24\"   \n",
    "\n",
    "def extract_features(path, sr=16000, n_mfcc=40):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0)\n",
    "\n",
    "def gender_from_actor(actor_id: int):\n",
    "    return 0 if actor_id % 2 == 1 else 1  \n",
    "\n",
    "def artificial_age_from_actor(actor_id: int):\n",
    "    if 1 <= actor_id <= 8:   return 25\n",
    "    if 9 <= actor_id <= 16:  return 45\n",
    "    if 17 <= actor_id <= 20: return 58\n",
    "    return 68  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3c979c-c71b-42b8-bbfb-06ec94609483",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_emo, test_size=0.2, random_state=42)\n",
    "\n",
    "emo_model = Sequential([\n",
    "    Input(shape=(40,)),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(8, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "emo_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "emo_model.fit(X_train, y_train,\n",
    "              validation_data=(X_test, y_test),\n",
    "              epochs=30, batch_size=32)\n",
    "\n",
    "emo_model.save(\"emotion_model.keras\")\n",
    "print(\" Emotion model saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d015472-86f7-4f2a-8890-57cf70b9cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "gender_model = load_model(\"gender_model.keras\")\n",
    "age_model = load_model(\"age_model.keras\")\n",
    "emotion_model = load_model(\"emotion_model.keras\")\n",
    "\n",
    "EMO_LABELS = [\"neutral\",\"calm\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgust\",\"surprised\"]\n",
    "\n",
    "def extract_features(path, sr=16000, n_mfcc=40):\n",
    "    y, _ = librosa.load(path, sr=sr)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
    "    return np.mean(mfcc.T, axis=0).astype(np.float32).reshape(1,-1)\n",
    "\n",
    "def predict(path):\n",
    "    feat = extract_features(path)\n",
    "\n",
    "    \n",
    "    g_pred = np.argmax(gender_model.predict(feat))\n",
    "    if g_pred == 1:\n",
    "        return {\"message\": \"Upload male voice.\"}\n",
    "\n",
    "    \n",
    "    age_pred = age_model.predict(feat)[0][0]\n",
    "    result = {\"gender\": \"male\", \"age\": round(age_pred,1)}\n",
    "\n",
    "    \n",
    "    if age_pred >= 60:\n",
    "        emo_pred = np.argmax(emotion_model.predict(feat))\n",
    "        result[\"emotion\"] = EMO_LABELS[emo_pred]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = r\"C:\\Users\\HP\\Downloads\\1001_IEO_HAP_HI.wav\"\n",
    "    print(predict(file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5e0f94-ef37-4ce2-afa1-e5a7bcf1ecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import librosa, numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "gender = load_model(\"gender_model.keras\")\n",
    "age = load_model(\"age_model.keras\")\n",
    "emo = load_model(\"emotion_model.keras\")\n",
    "emo_labels = [\"neutral\",\"calm\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgust\",\"surprised\"]\n",
    "\n",
    "def extract(f): \n",
    "    y,_=librosa.load(f,sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(y=y,sr=16000,n_mfcc=40)\n",
    "    return np.mean(mfcc.T,axis=0).reshape(1,-1)\n",
    "\n",
    "def predict(f):\n",
    "    x = extract(f)\n",
    "    if np.argmax(gender.predict(x))==1: \n",
    "        return \"Upload male voice\"\n",
    "    a = age.predict(x)[0][0]\n",
    "    msg = f\"Gender: Male | Age: {round(a)}\"\n",
    "    if a>=60: \n",
    "        msg += f\" | Emotion: {emo_labels[np.argmax(emo.predict(x))]}\"\n",
    "    return msg\n",
    "\n",
    "\n",
    "up = widgets.FileUpload(accept=\".wav\", multiple=False)\n",
    "display(up)\n",
    "\n",
    "def on_up(c): \n",
    "    for n,f in up.value.items():\n",
    "        with open(n,\"wb\") as out: out.write(f[\"content\"])\n",
    "        print(predict(n))\n",
    "\n",
    "up.observe(on_up,\"value\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e08c8-23c9-43da-b35e-779681151e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeacfc2-a77a-478e-a2be-b7a9ea3e3b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "upload = widgets.FileUpload(accept='.wav', multiple=False)\n",
    "display(upload)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a41eb4-daa6-4204-9288-f73c93aee972",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32369b8-ac14-4e32-a42a-4522ba0d7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b654500-d7fc-4a45-8f8e-16aed4d49716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcc8445-177b-49b9-9730-b0bb72005a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "gender_model = load_model(\"gender_model.keras\")\n",
    "age_model = load_model(\"age_model.keras\")\n",
    "emo_model = load_model(\"emotion_model.keras\")\n",
    "\n",
    "emo_labels = [\"neutral\",\"calm\",\"happy\",\"sad\",\"angry\",\"fearful\",\"disgust\",\"surprised\"]\n",
    "\n",
    "def plot_matrix(y_true, y_pred, labels, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           title=title,\n",
    "           ylabel=\"True label\",\n",
    "           xlabel=\"Predicted label\")\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], \"d\"),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.show()\n",
    "    print(classification_report(y_true, y_pred, target_names=labels))\n",
    "\n",
    "y_true = np.argmax(y_test_gender, axis=1)\n",
    "y_pred = np.argmax(gender_model.predict(X_test_gender), axis=1)\n",
    "plot_matrix(y_true, y_pred, [\"Male\",\"Female\"], \"Gender Detection\")\n",
    "\n",
    "y_true = np.digitize(y_test_age, bins=[30, 50, 70])  \n",
    "y_pred = np.digitize(age_model.predict(X_test_age).flatten(), bins=[30, 50, 70])\n",
    "plot_matrix(y_true, y_pred, [\"Young\",\"Adult\",\"Middle-aged\",\"Senior\"], \"Age Detection\")\n",
    "\n",
    "y_true = np.argmax(y_test_emo, axis=1)\n",
    "y_pred = np.argmax(emo_model.predict(X_test_emo), axis=1)\n",
    "plot_matrix(y_true, y_pred, emo_labels, \"Emotion Detection\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdddaac7-6d3f-44ac-804c-f2fa44381fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requirement tex\n",
    "\n",
    "#Tensorflow\n",
    "#keras\n",
    "#librosa\n",
    "#Numpy\n",
    "#scikit-learn\n",
    "#Matplotlib\n",
    "#seaborn\n",
    "#ipywidgets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed5db835-a1c2-47b6-8ded-3cb9784f43c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=gender_model.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mthreading\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m gender_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgender_model.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m age_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m emotion_model \u001b[38;5;241m=\u001b[39m load_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=gender_model.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import datetime\n",
    "from tensorflow.keras.models import load_model\n",
    "import threading\n",
    "import os\n",
    "\n",
    "gender_model = load_model(\"gender_model.keras\")\n",
    "age_model = load_model(\"age_model.keras\")\n",
    "emotion_model = load_model(\"emotion_model.keras\")\n",
    "\n",
    "log_file = \"audio_log.csv\"\n",
    "if not os.path.exists(log_file):\n",
    "    pd.DataFrame(columns=[\"Audio_File\",\"Gender\",\"Age\",\"Emotion\",\"Timestamp\"]).to_csv(log_file, index=False)\n",
    "\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs\n",
    "\n",
    "def process_audio(file_path):\n",
    "    features = extract_features(file_path).reshape(1,-1)\n",
    "    gender_pred = np.argmax(gender_model.predict(features))\n",
    "    gender = \"Male\" if gender_pred==0 else \"Female\"\n",
    "    if gender==\"Female\":\n",
    "        result_label.config(text=\"Upload male voice\")\n",
    "        return\n",
    "    age_pred = int(np.round(age_model.predict(features)[0][0]))\n",
    "    if age_pred>60:\n",
    "        emotion_pred = np.argmax(emotion_model.predict(features))\n",
    "        emotions = [\"Happy\",\"Sad\",\"Angry\",\"Neutral\"]\n",
    "        emotion = emotions[emotion_pred]\n",
    "    else:\n",
    "        emotion = \"-\"\n",
    "    ts = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_data = pd.DataFrame([[file_path,gender,age_pred,emotion,ts]], columns=[\"Audio_File\",\"Gender\",\"Age\",\"Emotion\",\"Timestamp\"])\n",
    "    existing = pd.read_csv(log_file)\n",
    "    updated = pd.concat([existing,new_data], ignore_index=True)\n",
    "    updated.to_csv(log_file, index=False)\n",
    "    if emotion==\"-\":\n",
    "        result_label.config(text=f\"Age: {age_pred}, Gender: {gender}\")\n",
    "    else:\n",
    "        result_label.config(text=f\"Age: {age_pred}, Gender: {gender}, Emotion: {emotion}\")\n",
    "\n",
    "def browse_file():\n",
    "    file_path = filedialog.askopenfilename(filetypes=[(\"Audio files\",\"*.wav;*.mp3\")])\n",
    "    if file_path:\n",
    "        t = threading.Thread(target=process_audio, args=(file_path,))\n",
    "        t.start()\n",
    "\n",
    "root = tk.Tk()\n",
    "root.title(\"Age and Emotion Detection from Audio\")\n",
    "root.geometry(\"400x200\")\n",
    "btn = tk.Button(root, text=\"Upload Audio\", command=browse_file, width=30, height=2, bg=\"lightblue\")\n",
    "btn.pack(pady=30)\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Helvetica\",12))\n",
    "result_label.pack(pady=10)\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae75e49f-6bc5-4a3c-a8df-97ff1eb158d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbf58490-e647-48a9-8b67-837e6dfe6707",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=gender_model.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gender_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgender_model.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=gender_model.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "gender_model = load_model(\"gender_model.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "950ecc6d-a3e0-4761-a2df-efa7c97b8cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bfb8006-765c-476e-bc2f-dbf4d06bd85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\longhairenv\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models saved successfully\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model_gender = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(40,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])\n",
    "model_gender.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_gender.save(\"gender_model.keras\")\n",
    "\n",
    "model_age = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(40,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation=\"linear\")\n",
    "])\n",
    "model_age.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n",
    "model_age.save(\"age_model.keras\")\n",
    "\n",
    "model_emotion = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(40,)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(4, activation=\"softmax\")\n",
    "])\n",
    "model_emotion.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model_emotion.save(\"emotion_model.keras\")\n",
    "\n",
    "print(\"Models saved successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "22b2c005-c291-4e97-ac24-75365bdc87c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV created at: C:\\Users\\HP\\audio_log.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = os.getcwd()\n",
    "log_file = os.path.join(base_path, \"audio_log.csv\")\n",
    "\n",
    "if not os.path.exists(log_file):\n",
    "    pd.DataFrame(columns=[\"Audio_File\",\"Gender\",\"Age\",\"Emotion\",\"Timestamp\"]).to_csv(log_file, index=False)\n",
    "\n",
    "print(\"CSV created at:\", log_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa8ca1f-d831-4d23-be28-681eae11c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (longhairenv)",
   "language": "python",
   "name": "longhairenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
